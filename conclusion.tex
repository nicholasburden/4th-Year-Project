\subsection{Summary of Results}

The main result from this report concerns training and testing on multiple scenarios using our task invariant architecture. We found that transferring to an unseen scenario exhibited very poor performance (although performance was less poor when transferring to a smaller map). However, we found that when the architecture is trained on multiple scenarios simultaneously, it exhibited high performance on each of those maps. This indicates that a single network has a far greater capacity for learning than a single scenario. Furthermore, this shows that the various tasks are in fact very closely linked; we have shown that these scenarios can be played very well by a generalisable architecture trained over each map, which supports our partial observability hypothesis.



Moreover, among our results were useful conclusions regarding the strength of various features of an agent architecture in this environment. We found that using actions as inputs into a convolutional encoder was an extremely successful technique, to the point that only a minimal observation input was required, since the action itself provided extremely valuable information.

Finally, we found that the memory requirements for architectures utilising grid-based observations and actions to be extremely large (~5Gb per network, compared to under 1Gb for non-grid-based architectures for small maps). This is a major downfall of the approach, as it limits the size the map that can be trained upon, and increases the training time.





\subsection{Future Work}

Due to the size and scope of this project there are a number of directions in which it can be continued or improved. We detail some of the most promising avenues here.


\subsubsection{Task Invariance Using QMIX}

The QMIX mixing network is inherently dependent on the task: the number of inputs in the network depends on the number of agents. The VDN mixing network does not have this problem, it simply sums the $Q$-values of each agent. Future work could involve adapting the QMIX mixing network for task invariance. This would involve a transformer-style architecture to process a new representation of the state that is independent of the number of units in the map.


\subsubsection{Observing Terrain}

The terrain of a StarCraft II map (high ground and low ground) can be particularly important information to aid with micromanagement. For example, attacking from high ground allows units to not be seen by the enemy (until the enemy is attacked), so it is an advantageous position to be in.

Currently, the agent architecture does not use this observation, but it would be useful to evaluate its effectiveness.


\subsection{Personal Development}
This project allowed me to explore my passion for Reinforcement Learning. In particular, I was able to implement and appreciate various concepts from the Machine Learning course, such as convolutional neural networks, as well as evaluating them using state-of-the-art hardware.

Furthermore, the scope of this project was larger than anything previously undertaken, which gave me an invaluable insight into collaborative research by using open-communication channels, version control of a large shared repository, as well as discussions with various other lab members. This project has also been used by the lab to work towards a more general multi-agent RL algorithm.