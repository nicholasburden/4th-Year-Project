\subsection{Summary of Results}

The main result from this report concerns training and testing on multiple scenarios using our task-invariant architecture. We found that transferring to an unseen scenario exhibited very poor performance (although performance was less poor when transferring to a smaller map). This is likely because of the ``out-of-distribution'' \cite{ood} problem: faced with a sample outside the training distribution, as neural network will generalise arbitrarily badly. This is particularly relevant to RNNs, where one observation difference in the entire history can lead to completely different hidden states. This also supports why we saw a slight improvement when transferring from larger to smaller maps: if the network has seen similar states, the transfer is more successful.



However, we found that multi-task learning was more successful, suggesting that this ``out-of-distribution'' problem is resolved through this style of learning. Furthermore, this indicates that a single network has a far greater capacity for learning than a single scenario. Moreover, this shows that the various tasks are in fact closely linked: we have shown that these scenarios can be played somewhat well by a generalisable architecture trained over each map, which supports our partial observability hypothesis (Hypothesis 1).



Finally, we found that the memory requirements for architectures utilising grid-based observations and actions to be extremely large (~5Gb per network, compared to under 1Gb for non-grid-based architectures for small maps). This is a major downfall of the approach, as it limits the size the map that can be trained upon, and increases the training time. This may require the use of a smaller replay buffer or grid resolution, which may decrease performance.





\subsection{Future Work}

Due to the size and scope of this project there are a number of directions in which it can be continued or improved. We detail some of the most promising avenues here.


\subsubsection{Task Invariance Using QMIX}

The QMIX mixing network is inherently dependent on the task: the number of inputs in the network depends on the number of agents. The VDN mixing network does not have this problem, it simply sums the $Q$-values of each agent. Future work could involve adapting the QMIX mixing network for task invariance. This would involve a transformer-style architecture to process a new representation of the state that is independent of the number of units in the map.


\subsubsection{Observing Terrain}

The terrain of a StarCraft II map (high ground and low ground) can be particularly important information to aid with micromanagement. For example, attacking from high ground allows units to not be seen by the enemy (until the enemy is attacked), so it is an advantageous position to be in.

Currently this observation is not input into the network, but it would be useful to evaluate its effectiveness.


\subsection{Personal Development}
This project allowed me to explore my passion for Reinforcement Learning. In particular, I was able to implement and appreciate various concepts from the Machine Learning course, such as convolutional neural networks, as well as evaluating them using state-of-the-art hardware.

Furthermore, the scope of this project was larger than anything previously undertaken, which gave me an invaluable insight into collaborative research by using open-communication channels, version control of a large shared repository, as well as discussions with various other lab members. This project has also been used by the lab to work towards a more general multi-agent RL algorithm.