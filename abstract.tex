Deep Multi-Agent Reinforcement Learning (DMARL) is a rapidly advancing subfield within AI owing to its ability to learn end-to-end control policies from high-dimensional input states, allowing the acquisition of super-human game play skills across a variety of domains. One particularly prominent benchmark suite in the space of partially observable multi-agent games is the StarCraft Multi-Agent Challenge (SMAC) \cite{smac}, which is based on challenging StarCraft II unit micromanagement tasks. 
While SMAC tasks are diverse, they do draw from a small finite set of unit types and give rise to locally similar observations. We therefore hypothesise that it should be possible to both train policies that can generalise over previously unseen tasks, as well to warm-start training of difficult large-scale tasks using policy weights from similar small-scale tasks.
However, the state-of-the-art multi-agent learning algorithm QMIX \cite{qmixcite} has a network architecture that may admit a different number of weights for tasks with different numbers of agents and enemies.  
In order to construct a universal network architecture for QMIX's utility functions, we first equip it with grid input representations and then perform a comprehensive investigation of the relative performance of a variety of convolutional network architectures. We find that combining grid-based input representations with an additive mixing function (VDN \cite{vdn}) helps construct a universal multi-agent architecture that better generalises over scenarios with different numbers of agents and enemies and that, surprisingly, good training performance can be reached with much less input information than previously assumed.