Deep Multi-Agent Reinforcement Learning (DMARL) is a rapidly advancing subfield within AI owing to its ability to learn end-to-end control policies from high-dimensional input states, allowing to acquire super-human game play skills across a variety of domains. One particularly important benchmark suite in the space of partially observable multi-agent games is SMAC, which is based on challenging StarCraftII unit micromanagement tasks. 
While SMAC tasks are diverse, they do draw from a small finite set of unit types and give rise to locally similar observations. We therefore hypothesise that it should be possible to both train policies that can generalise over previously unseen tasks, as well to warm-start training of difficult large-scale tasks using policy weights from similar small-scale tasks.
However, state-of-the-art multi-agent learning algorithm QMIX has a network architecture that may admit different numbers of weights for tasks with different numbers of agents and enemies.  
In order to construct a universal network architecture for QMIX, we first equip it with grid input representations and then perform a comprehensive investigation of the relative performance of a variety of convolutional network architectures. We find that grid-based input representations allow QMIX architecture to better generalise over scenarios with different numbers of agents and enemies and that, surprisingly, good training performance can be reached with much less input information than previously assumed.