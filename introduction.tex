Reinforcement Learning (RL) is an extremely relevant and successful approach to many modern problems. In recent years, great proficiency in complex tasks such as Go \cite{go} and Resource Management \cite{resourcemanagement} has been achieved using RL. These successes are important as the RL algorithms employed are general, and can easily be applied to other domains, while more traditional approaches to AI are specific to the problem at hand. 

Multi-agent RL is a natural next step, where multiple agents must co-operate to achieve a common goal. Some success has already been made in this space, examples being a multi-agent approach to traffic light co-ordination systems \cite{traffic} and (more specific to the domain of this project) superhuman proficiency in Starcraft II \cite{alphastar}, a popular real-time strategy video game. These advances are incredibly relevant to AI, as the world is inherently multi-agent: almost every AI problem can be viewed as a co-operation task between various actors. Success in this domain is integral to the advancement of AI and machine learning as a whole.

This project focuses on the role of multi-agent RL in Starcraft II video game in the form of a general RL algorithm for multiple co-operating agents. A key part of this project will be working towards an approach that is independent of the number of units in the map, a feature that is not seen in \cite{alphastar}. This is important as it makes the RL algorithm more general, and allows for transfer learning to different scenarios. This will involve a new approach to a state and action abstraction, as well as comprehensive experiments to determine the viability of these approaches, and compare their performance to standard approaches.

\subsection{The Challenge of Starcraft}

Starcraft II is a real-time strategy video game made by Blizzard Entertainment. It is a challenging game that has been played in esports tornaments for over 20 years.

Games are generally played with one player against another, each choosing to play as an alien race: Zerg, Protoss or Terran, with each having different charcacteristics. The game involves using workers to gather resources, which can make more units and buildings, as well as using other units to attack the enemy player. In this way, a successful player is able to manage the long-term overall structure of their team, know as macromanagement, as well as manage the lower level actions of individual units, know as micromanagement. This project will focus on micromanagement, as this is generally seen as a limiting factor for the competency of a player.

The game is challenging for a number of reasons. Firstly, as with game theory, the state of the game is constantly altered by both players, so there is no ``best strategy''. This style of game requires constant adaptation to the opponent's move, and must be responsive in this way to succeed. 

Furthermore, each agent suffers from a lack of information. In the implementation of the environment we will use, each agent has a limited field of view, so must make use of what it can, and has historically, observed. To overcome this problem, this project will explore the use of recurrent neural networks, which allow for past data to be used to make a present decision.

Starcraft is a difficult application of reinforcement learning as the action space can be extremely large. As well as moving to any place on the map, the unit is able to attack any enemy unit within its field of view, and can also perform special moves, such as healing. Approaches to overcome this problem must be made, such as restricting movement to one of four directions at a time, in order to reduce the size of this action space. 

Finally, as mentioned before, a significant part of this project will be working towards an architecture that is independent of the number of units in the game. The action space is dependent on this, since for each enemy unit, the agent has an attack action. Other observations, such as the agent's identifier, are also dependent on the number of agents (each identifier is represented as a binary string), which is also undesirable as it reduces generality. A key goal of this project is to allow for transfer learning over different scenarios, for which we must rid of this dependency.

\subsection{Recent Work}

Due to its difficulty, Starcraft II is an attractive domain for Machine Learning (ML). AlphaStar has demonstrated success in this domain \cite{alphastar}, recently beating one of the top professional players, Team Liquidâ€™s Grzegorz ``MaNa'' Komincz, 5-0. AlphaStar uses deep neural networks for multi-agent reinforcement learning, training using data from both human and data games \cite{alphastar}. AlphaStar uses neural network to represent a central policy, which is also conditioned over a statistic $z$ that summarizes a strategy sampled from human data \cite{alphastar}. Overall, AlphaStar has achieved grandmaster status in all three races, demonstrating its adaptability to different scenarios.


Furthermore, recent work by the Whiteson Research Lab (WhiRL) has demonstrated success in multi-agent RL. The lab's PyMarl framework has implementations of various multi-agent RL algorithms, including QMIX \cite{qmix}, a state-of-the-art value-based method that can train decentralised policies in a centralised end-to-end fashion \cite{qmix}. QMIX has demonstrated significantly improved performance compared to other value-based methods \cite{qmix}, such as IQL \cite{IQL} and VDN \cite{vdn}, in the Starcraft II environment. QMIX, as well as the VDN, will be used as the value-based methods in this project, due to their great performance in this environment.

\subsection{Project Aims}

The main aim of this project is to explore new ways of abstracting states and actions, and compare their efficacy against standard approaches. These abstractions will take the form of an image (i.e. a 2-dimensional tensor with various channels encoding information) representing the geographical layout of the environment from an agent's point of view. This will be done with the aim of removing the dependence of the action space on the number of units, as mentioned before. Achieving this will also allow for a more generalizable framework that can learn in a multitude of complex environments. Conducting experiments with agent networks that use these abstractions will also allow us to fine tune the parameters specific to this idea, which is an important process in itself.

Furthermore, another significant aim of this project is to explore different architectures for agent networks, and evaluate their relative performance. This will allow for conclusions to be made about what features of an agent network give the best performance for a multi-agent system in the Starcraft II environment.

