Advances in Reinforcement Learning (RL) have recently resulted in great success across a number of difficult control tasks, such as autonomous driving \cite{driving}, Atari games \cite{dqn} and Resource Management \cite{resourcemanagement}. Deep RL allows us to solve high-dimensional control tasks end-to-end in a principled, transferable way.

Multi-Agent Reinforcement Learning (MARL) is a natural next step, where multiple agents must learn to cooperate or compete with one another. Within this project we are only concerned with cooperative multi-agent tasks. 

A large amount of success has already been made in this space, examples being a multi-agent approach to traffic light coordination systems \cite{traffic} and (more specific to the domain of this project) superhuman proficiency in StarCraft II, a popular real-time strategy video game, with DeepMind's AlphaStar AI \cite{alphastar}. These advances are incredibly relevant to AI, as the world is inherently multi-agent: many AI problems are cooperation tasks between various actors. Success in this domain is integral to the advancement of AI and machine learning as a whole.

In some environments (in particular, StarCraft II), each agent may not be able to fully observe the system state (\textit{partial observability}). One possible solution to this is a paradigm referred to as \textit{decentralised execution}: agents are unable to fully communicate during execution, and therefore must operate alone. However, during training, agents are able to learn together by a centralised coordinator, giving joint rewards for joint actions, as we assume that the global state is available. This gives the paradigm of \textit{centralised training with decentralised execution}.







This project focuses on designing and evaluating MARL network architectures for StarCraft II micromanagement tasks, with the goal of making an architecture invariant to the scenario faced by the agents. We make the following hypothesis:

\begin{hyp}[Partial Observability] \label{hyp:first}
Due to the partial observability of StarCraft II, scenarios involving similar units (but possibly a different number of them) look the same to a neural network trained on various tasks within the game.
\end{hyp}

According to this hypothesis, such a network would exhibit good performance on multiple ``similar'' tasks.

Such task invariance is not seen in DeepMind's AlphaStar \cite{alphastar}, and would require a new approach to state and action abstraction, as well as comprehensive experiments to determine the viability of these approaches and to compare their performance to standard approaches.


\subsection{The Challenge of StarCraft}

StarCraft II is a real-time strategy video game developed by Blizzard Entertainment. It is a challenging game that has been played in e-sports tournaments for over 20 years.

Games are generally played as 2-player adversarial games, each player choosing to play as an alien race: Zerg, Protoss or Terran, each having different characteristics. The game involves using workers to gather resources, which can make more units and buildings, as well as using other units to attack the enemy's units and buildings. In this way, a successful player is able to manage the long-term overall structure of their assets, know as macromanagement, as well as manage the lower level actions of individual units, know as micromanagement. This project will focus on micromanagement, which contributes significantly to the success of a player.

The game is challenging for a number of reasons. Firstly, the state of the game is constantly altered by both players, so there is often no clear superior move. This style of game requires constant adaptation to the opponent's moves, and the player must be responsive in this way to succeed. 

Furthermore, each unit suffers from a lack of information. In the implementation of the environment we will use, each agent has a limited field of view, so must make use of what it can, and what it has historically observed. To overcome this problem, this project will explore the use of recurrent neural networks, which allow for past data to be used to make decisions relevant to the present moment.

StarCraft II is a difficult application of RL as the action space can be extremely large. As well as moving to any place on the map, a unit is able to attack any enemy unit within its field of view, and can also perform special moves, such as healing. Approaches to overcome this problem must be made, such as restricting movement to cardinal directions at a time, in order to reduce the size of this action space. 

Finally, as mentioned previously, a significant part of this project will be working towards a network architecture that is independent of the number of units in the game. The action space is dependent on this, since for each enemy unit, an agent has an attack action. Other observations, such as an agent's unique ID, are also dependent on the number of agents (each ID is represented as a binary string), which is also undesirable as it reduces generality. A key goal of this project is to allow for transfer learning over different scenarios, for which we must remove this dependency.

\subsection{Recent Work}

Due to its difficulty, StarCraft II is an attractive domain for Machine Learning (ML). AlphaStar (DeepMind's MARL AI for StarCraft II) has demonstrated success in this domain \cite{alphastar}, recently beating one of the top professional players, Team Liquidâ€™s Grzegorz ``MaNa'' Komincz, 5-0. AlphaStar uses deep neural networks for MARL, training using data from both human and agent games \cite{alphastar}, to represent a central policy which is also conditioned over a statistic $z$ that summarises a strategy sampled from human data \cite{alphastar}. Overall, AlphaStar has achieved ``grandmaster'' status (ranking in the 99.8th percentile) in all three races, demonstrating its adaptability to different scenarios.


Furthermore, recent work by the Whiteson Research Lab (WhiRL) has demonstrated success in MARL. The lab's PyMARL framework \cite{smac} has implementations of various MARL algorithms, including QMIX \cite{qmixcite}, a state-of-the-art value-based method that can train decentralised policies in a centralised end-to-end fashion \cite{qmixcite}. QMIX has demonstrated significantly improved performance compared to other value-based methods \cite{qmixcite}, such as IQL \cite{IQL} and VDN \cite{vdn}, in the StarCraft II environment. QMIX, as well as VDN, will be used as the value-based methods in this project, due to their great performance in this environment.

Finally, some work has recently explored the idea of using a graph to represent the environment \cite{graph}, with each node representing an agent, each connected to its K nearest neighbours via edges. A convolutional encoder is then used to extract spatial information from this graph in order to learn cooperation between agents. This paper demonstrates that this idea substantially improves upon existing ideas in the field \cite{graph}. However, the architecture remains dependent upon the number of agents, and is thus not task invariant.



\subsection{Project Aims}

The main aim of this project is to explore new ways of abstracting both states and actions in order to produce a task invariant network architecture, and compare their efficacy against standard approaches. These abstractions will take the form of an image (i.e. a 2-dimensional tensor with various channels encoding information) representing the geographical layout of the environment from an agent's perspective. Achieving this will also allow for a more generalisable framework that can learn in a multitude of complex environments, and therefore have high performance across many tasks. 


