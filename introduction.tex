Reinforcement Learning (RL) is an extremely relevant area and successful approach to many modern problems \cs{vague first sentence. Better: Advances in RL have recently resulted in great advances across a number of difficult control tasks, such as autonomous driving etc (give a number of domains and references here)}. In recent years, great proficiency in complex tasks such as Atari games \cite{dqn} and Resource Management \cite{resourcemanagement} has been achieved using RL. These successes are important as the RL algorithms employed are general, and can easily be applied to other domains, while more traditional approaches to artificial intelligence (AI) are specific to the problem at hand. \cs{Mhh. I would restrict this to control tasks: DRL allows to solve high-dimensional control tasks end-to-end in a principled, tranferrable way.}

Multi-agent RL is a natural next step, where multiple agents must learn to cooperate or compete with one another. Within this project we are only concerned with cooperative multi-agent tasks. 

A large amount of success has already been made in this space, examples being a multi-agent approach to traffic light co-ordination systems \cite{traffic} and (more specific to the domain of this project) superhuman proficiency in StarCraft II \cite{alphastar}, a popular real-time strategy video game. These advances are incredibly relevant to AI, as the world is inherently multi-agent: many AI problems are cooperation tasks between various actors. Success in this domain is integral to the advancement of AI and machine learning as a whole.

In some environments (in particular, StarCraft II), each agent may not be able to fully observe the system state (\textit{partial observability}). One possible solution to this is a paradigm referred to as \textit{decentralised execution}: agents are unable to fully communicate during execution, and therefore must operate alone. However, during training, agents are able to learn together by a centralised coordinator, giving joint rewards for joint actions, as we assume that the global state is available. This gives the paradigm of \textit{centralised training with decentralised execution}.







This project focuses on designing and evaluating multi-agent RL network architectures for StarCraft II, with the goal of making an architecture invariant to the scenario faced by the agent. We hypothesise that due to the partial observability of StarCraft II, scenarios involving similar units (but possibly a different number of them) look the same to a neural network trained on various tasks within the game. In this report, we refer to this hypothesis as the \textit{partial observability hypothesis}. A way to test this hypothesis is through evaluating an architectures performance on unseen tasks, which will form a significant part of the evaluation section of this project.

Such task invariance is not seen in DeepMind's AlphaStar \cite{alphastar} (a Starcraft II AI using deep multi-agent RL), and would require a new approach to state and action abstraction, as well as comprehensive experiments to determine the viability of these approaches and compare their performance to standard approaches.


\subsection{The Challenge of StarCraft}

StarCraft II is a real-time strategy video game developed by Blizzard Entertainment. It is a challenging game that has been played in e-sports tournaments for over 20 years.

Games are generally played with one player against another \cs{Better: 2-player adversarial game}, each choosing to play as an alien race: Zerg, Protoss or Terran, with each having different characteristics. The game involves using workers to gather resources, which can make more units and buildings, as well as using other units to attack the enemy's units and buildings. In this way, a successful player is able to manage the long-term overall structure of their assets, know as macromanagement, as well as manage the lower level actions of individual units, know as micromanagement. This project will focus on micromanagement, as this is generally seen as a limiting factor for the competency of a player.

The game is challenging for a number of reasons. Firstly, the state of the game is constantly altered by both players, so there is no ``best'' strategy \cs{mhh. not sure if that is true though, there might be dominating strategies particularly if you have badly balanced scenarios?}. This style of game requires constant adaptation to the opponent's moves, and the player must be responsive in this way to succeed. 

Furthermore, each unit suffers from a lack of information. In the implementation of the environment we will use, each agent has a limited field of view, so must make use of what it can, and has historically, observed. To overcome this problem, this project will explore the use of recurrent neural networks, which allow for past data to be used to make decisions relevant to the present moment

StarCraft is a difficult application of reinforcement learning as the action space can be extremely large. As well as moving to any place on the map, the unit is able to attack any enemy unit within its field of view, and can also perform special moves, such as healing. Approaches to overcome this problem must be made, such as restricting movement to cardinal directions at a time, in order to reduce the size of this action space. 

Finally, as mentioned previously, a significant part of this project will be working towards an architecture that is independent of the number of units in the game. The action space is dependent on this, since for each enemy unit, the agent has an attack action. Other observations, such as the agent's identifier, are also dependent on the number of agents (each identifier is represented as a binary string), which is also undesirable as it reduces generality. A key goal of this project is to allow for transfer learning over different scenarios, for which we must remove this dependency.

\subsection{Recent Work}

Due to its difficulty, StarCraft II is an attractive domain for Machine Learning (ML). AlphaStar has demonstrated success in this domain \cite{alphastar}, recently beating one of the top professional players, Team Liquidâ€™s Grzegorz ``MaNa'' Komincz, 5-0. AlphaStar uses deep neural networks for multi-agent reinforcement learning, training using data from both human and agent games \cite{alphastar}. AlphaStar uses neural network to represent a central policy, which is also conditioned over a statistic $z$ that summarises a strategy sampled from human data \cite{alphastar}. Overall, AlphaStar has achieved ``grandmaster'' status (ranking in the 99.8th percentile) in all three races, demonstrating its adaptability to different scenarios.


Furthermore, recent work by the Whiteson Research Lab (WhiRL) has demonstrated success in multi-agent RL. The lab's PyMARL framework has implementations of various multi-agent RL algorithms, including QMIX \cite{qmixcite}, a state-of-the-art value-based method that can train decentralised policies in a centralised end-to-end fashion \cite{qmixcite}. QMIX has demonstrated significantly improved performance compared to other value-based methods \cite{qmixcite}, such as IQL \cite{IQL} and VDN \cite{vdn}, in the StarCraft II environment. QMIX, as well as the VDN, will be used as the value-based methods in this project, due to their great performance in this environment.

Finally, some work has recently explored the idea of using a graph to represent the environment \cite{graph}, with each node representing an agent, each connected to its K nearest neighbours via edges. A convolutional encoder is then used to extract spatial information from this graph in order to learn cooperation between agents. This paper demonstrates that this idea substantially improves upon existing ideas in the field \cite{graph}. However, the architecture remains dependent upon the number of agents, and is thus not task invariant.



\subsection{Project Aims}

The main aim of this project is to explore new ways of abstracting both states and actions in order to produce a task-invariant network architecture, and compare their efficacy against standard approaches. These abstractions will take the form of an image (i.e. a 2-dimensional tensor with various channels encoding information) representing the geographical layout of the environment from an agent's perspective. Achieving this will also allow for a more generalisable framework that can learn in a multitude of complex environments, and therefore have high performance across many tasks. 


